# work-demo

This project is a demonstration of a data processing and analysis pipeline for SEC filings, showcasing both exploratory data analysis (EDA) and a generative AI-powered semantic search.

## Features

- **EDA Pipeline:** A PySpark-based pipeline that processes SEC filings from the `edgar-corpus` dataset on HuggingFace. It performs data loading, chunking, embedding generation, dimensionality reduction, clustering, and visualization.
- **Generative AI Pipeline:** A Python-based pipeline that uses a sentence transformer model to perform semantic search on SEC filings. It includes data loading, chunking, embedding creation, and a validation process.
- **Semantic Search:** The ability to search for specific information within the SEC filings using natural language queries.
- **Visualization:** The EDA pipeline generates several plots to visualize the clustered data, including cluster assignments, outliers, and section-based coloring.

## Project Structure

```
.
├��─ eda
│   ├── main.py
│   ├── plots
│   └── src
│       ├── chunking.py
│       ├── clustering.py
│       ├── data_loader.py
│       ├── embeddings.py
│       └── visualization.py
└── genai
    ├── data_loader.py
    └── main.py
```

- **`eda/`**: Contains the PySpark-based EDA pipeline.
  - **`main.py`**: The main script to run the EDA pipeline.
  - **`src/`**: Contains the source code for the different steps of the EDA pipeline.
  - **`plots/`**: The output directory for the visualizations generated by the EDA pipeline.
- **`genai/`**: Contains the Python-based generative AI pipeline for semantic search.
  - **`data_loader.py`**: A script to load and filter the SEC filings data.
  - **`main.py`**: The main script to run the semantic search and validation.

## EDA Pipeline

The EDA pipeline is implemented using PySpark and consists of the following steps:

1.  **Data Loading:** Loads the `edgar-corpus` dataset from HuggingFace for the year 2020, filters it for 5 unique companies, and converts it to a Spark DataFrame.
2.  **Chunking:** Splits the documents into sections and then further splits long sections into chunks of approximately 500 words.
3.  **Embedding Generation:** Generates embeddings for each chunk using the `all-MiniLM-L6-v2` sentence transformer model.
4.  **Clustering:** Applies StandardScaler, PCA for dimensionality reduction, and KMeans for clustering. It also performs outlier detection.
5.  **Visualization:** Creates and saves several plots to visualize the 2D embeddings, colored by cluster assignment, outlier flag, section name, and company.

## Generative AI Pipeline

The generative AI pipeline is implemented in Python and uses the `sentence-transformers` library for semantic search.

1.  **Data Loading:** Loads pre-filtered SEC filing data from a CSV file.
2.  **Chunking:** Splits the documents into chunks of 500 words, focusing on specific sections (1, 1A, 7, 8, 10).
3.  **Embedding Creation:** Creates embeddings for each chunk using the `all-MiniLM-L6-v2` model.
4.  **Semantic Search:** Performs a semantic search using cosine similarity to find the most relevant chunks for a given query and year.
5.  **Validation:** Includes a validation process to demonstrate that the model can retrieve the correct information for a set of predefined questions.

## Setup and Usage

1.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

2.  **Run the EDA pipeline:**
    ```bash
    python eda/main.py
    ```

3.  **Run the Generative AI pipeline:**
    First, run the data loader to get the filtered data:
    ```bash
    python genai/data_loader.py
    ```
    Then, run the main script:
    ```bash
    python genai/main.py
    ```

## Future Improvements

### EDA Pipeline

- **Optimal Cluster Number:** The number of clusters in KMeans is hardcoded. This can be improved by using methods like the **Elbow Method** or **Silhouette Score** to find the optimal number of clusters.
- **Advanced Dimensionality Reduction:** For visualization, **UMAP** or **t-SNE** could be used instead of PCA to generate more meaningful 2D representations of the embeddings.
- **Smarter Chunking:** The current fixed-size word chunking can be improved by using sentence-based splitting or recursive chunking with overlap to better preserve the semantic context.
- **Reproducibility:** The data loading can be made deterministic by selecting specific companies or sorting them before selection.
- **Scalable Visualization:** The visualization step can be made more scalable by sampling the data before converting it to a Pandas DataFrame.
- **Configuration Management:** Hardcoded parameters could be externalized to a configuration file to make the pipeline more flexible.

### Generative AI Pipeline

The `genai/main.py` script includes several suggestions for future improvements:

- **Chunking Strategy:** The current fixed-size chunking can be improved by using more sophisticated methods like recursive character text splitting, adding overlap between chunks, or using semantic chunking based on sentence boundaries.
- **Search Strategy:** The search process can be enhanced by using re-ranking with a more powerful cross-encoder model, query expansion, or a hybrid search approach that combines semantic and keyword-based search.
- **Model Selection:** Using a model fine-tuned on financial or legal documents could yield better results.
